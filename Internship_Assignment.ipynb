{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Internship Assignment",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Etk0INVcDiVX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "d25ae7c2-ce3b-4ccf-a23e-60ed85009953"
      },
      "source": [
        "g_drive_path = '/content/gdrive'\n",
        "from google.colab import drive\n",
        "drive.mount(g_drive_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfYUb5bx3nmd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "ad260693-1908-4f08-c959-e766b2a64511"
      },
      "source": [
        "!git clone https://github.com/sanyam83/text_renderer.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'text_renderer'...\n",
            "remote: Enumerating objects: 59, done.\u001b[K\n",
            "remote: Counting objects: 100% (59/59), done.\u001b[K\n",
            "remote: Compressing objects: 100% (56/56), done.\u001b[K\n",
            "remote: Total 712 (delta 27), reused 0 (delta 0), pack-reused 653\u001b[K\n",
            "Receiving objects: 100% (712/712), 11.62 MiB | 12.90 MiB/s, done.\n",
            "Resolving deltas: 100% (339/339), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8MAtpOL4Z3x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "f30d9885-b3eb-4630-ef43-5c92992545f9"
      },
      "source": [
        "%cd /content/text_renderer\n",
        "!python setup.py develop"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/text_renderer\n",
            "running develop\n",
            "running egg_info\n",
            "creating text_renderer.egg-info\n",
            "writing text_renderer.egg-info/PKG-INFO\n",
            "writing dependency_links to text_renderer.egg-info/dependency_links.txt\n",
            "writing top-level names to text_renderer.egg-info/top_level.txt\n",
            "writing manifest file 'text_renderer.egg-info/SOURCES.txt'\n",
            "writing manifest file 'text_renderer.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "Creating /usr/local/lib/python3.6/dist-packages/text-renderer.egg-link (link to .)\n",
            "Adding text-renderer 0.0.1 to easy-install.pth file\n",
            "\n",
            "Installed /content/text_renderer\n",
            "Processing dependencies for text-renderer==0.0.1\n",
            "Finished processing dependencies for text-renderer==0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vr-Nm2Ah4j4f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8f2229dd-4a84-411a-89d1-66e2a4fef075"
      },
      "source": [
        "!pip install -r docker/requirements.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting opencv-python==3.4.5.20\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/e1/d3eed618272f4b746339af1a84b2511e79c1708d88a9195cf25d743fa614/opencv_python-3.4.5.20-cp36-cp36m-manylinux1_x86_64.whl (25.4MB)\n",
            "\u001b[K     |████████████████████████████████| 25.4MB 119kB/s \n",
            "\u001b[?25hCollecting fontTools==4.12.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/80/6703106d6e3a56d0b124a2a1b3159d70ef383282b428a385bbee39762030/fonttools-4.12.1-py3-none-any.whl (803kB)\n",
            "\u001b[K     |████████████████████████████████| 808kB 57.1MB/s \n",
            "\u001b[?25hCollecting lmdb==0.98\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/5c/d56dbc2532ecf14fa004c543927500c0f645eaca8bd7ec39420c7546396a/lmdb-0.98.tar.gz (869kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 49.4MB/s \n",
            "\u001b[?25hCollecting pillow==6.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/fd/bbbc569f98f47813c50a116b539d97b3b17a86ac7a309f83b2022d26caf2/Pillow-6.2.2-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 52.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r docker/requirements.txt (line 5)) (1.18.5)\n",
            "Collecting tenacity\n",
            "  Downloading https://files.pythonhosted.org/packages/b5/05/ff089032442058bd3386f9cd991cd88ccac81dca1494d78751621ee35e62/tenacity-6.2.0-py2.py3-none-any.whl\n",
            "Collecting fire\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/a7/0e22e70778aca01a52b9c899d9c145c6396d7b613719cd63db97ffa13f2f/fire-0.3.1.tar.gz (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r docker/requirements.txt (line 8)) (4.41.1)\n",
            "Collecting loguru\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/4f/baee593c195cd4b56cf008c9473347f3b0795b47d3b946e03706a8b43fca/loguru-0.5.1-py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from -r docker/requirements.txt (line 10)) (1.14.2)\n",
            "Collecting sphinx==3.1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/bb/bbe3f555751da411ed892526d0d78ed1b9ff2a25ed2b0dab9f2670c8da07/Sphinx-3.1.2-py3-none-any.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 42.8MB/s \n",
            "\u001b[?25hCollecting sphinx-rtd-theme\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/86/1addf25a238bbd8466bb099f23d9a9f13494b22b37b44f6c41a778b8730f/sphinx_rtd_theme-0.5.0-py2.py3-none-any.whl (10.8MB)\n",
            "\u001b[K     |████████████████████████████████| 10.8MB 143kB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from -r docker/requirements.txt (line 13)) (0.7)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from -r docker/requirements.txt (line 14)) (3.6.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from tenacity->-r docker/requirements.txt (line 6)) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire->-r docker/requirements.txt (line 7)) (1.1.0)\n",
            "Collecting aiocontextvars>=0.2.0; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/db/c1/7a723e8d988de0a2e623927396e54b6831b68cb80dce468c945b849a9385/aiocontextvars-0.2.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->-r docker/requirements.txt (line 10)) (2.20)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from sphinx==3.1.2->-r docker/requirements.txt (line 11)) (2.11.2)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx==3.1.2->-r docker/requirements.txt (line 11)) (2.8.0)\n",
            "Collecting sphinxcontrib-htmlhelp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/8222554b29b3acde8420128d6d3999c5904d40922ef4b6ccb370e2be7421/sphinxcontrib_htmlhelp-1.0.3-py2.py3-none-any.whl (96kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 14.0MB/s \n",
            "\u001b[?25hCollecting sphinxcontrib-jsmath\n",
            "  Downloading https://files.pythonhosted.org/packages/c2/42/4c8646762ee83602e3fb3fbe774c2fac12f317deb0b5dbeeedd2d3ba4b77/sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: docutils>=0.12 in /usr/local/lib/python3.6/dist-packages (from sphinx==3.1.2->-r docker/requirements.txt (line 11)) (0.15.2)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx==3.1.2->-r docker/requirements.txt (line 11)) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from sphinx==3.1.2->-r docker/requirements.txt (line 11)) (49.6.0)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from sphinx==3.1.2->-r docker/requirements.txt (line 11)) (2.23.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx==3.1.2->-r docker/requirements.txt (line 11)) (2.1.3)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx==3.1.2->-r docker/requirements.txt (line 11)) (1.2.0)\n",
            "Collecting sphinxcontrib-applehelp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/47/86022665a9433d89a66f5911b558ddff69861766807ba685de2e324bd6ed/sphinxcontrib_applehelp-1.0.2-py2.py3-none-any.whl (121kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 60.2MB/s \n",
            "\u001b[?25hCollecting sphinxcontrib-qthelp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/14/05f9206cf4e9cfca1afb5fd224c7cd434dcc3a433d6d9e4e0264d29c6cdb/sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl (90kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 13.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx==3.1.2->-r docker/requirements.txt (line 11)) (20.4)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx==3.1.2->-r docker/requirements.txt (line 11)) (0.7.12)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.6/dist-packages (from sphinx==3.1.2->-r docker/requirements.txt (line 11)) (1.1.4)\n",
            "Collecting sphinxcontrib-devhelp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/09/5de5ed43a521387f18bdf5f5af31d099605c992fd25372b2b9b825ce48ee/sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl (84kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 13.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->-r docker/requirements.txt (line 14)) (8.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->-r docker/requirements.txt (line 14)) (20.1.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->-r docker/requirements.txt (line 14)) (1.9.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->-r docker/requirements.txt (line 14)) (0.7.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->-r docker/requirements.txt (line 14)) (1.4.0)\n",
            "Collecting contextvars==2.4; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/83/96/55b82d9f13763be9d672622e1b8106c85acb83edd7cc2fa5bc67cd9877e9/contextvars-2.4.tar.gz\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->sphinx==3.1.2->-r docker/requirements.txt (line 11)) (1.1.1)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.6/dist-packages (from babel>=1.3->sphinx==3.1.2->-r docker/requirements.txt (line 11)) (2018.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.5.0->sphinx==3.1.2->-r docker/requirements.txt (line 11)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.5.0->sphinx==3.1.2->-r docker/requirements.txt (line 11)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.5.0->sphinx==3.1.2->-r docker/requirements.txt (line 11)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.5.0->sphinx==3.1.2->-r docker/requirements.txt (line 11)) (2020.6.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->sphinx==3.1.2->-r docker/requirements.txt (line 11)) (2.4.7)\n",
            "Collecting immutables>=0.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/e0/ea6fd4697120327d26773b5a84853f897a68e33d3f9376b00a8ff96e4f63/immutables-0.14-cp36-cp36m-manylinux1_x86_64.whl (98kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 13.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: lmdb, fire, contextvars\n",
            "  Building wheel for lmdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lmdb: filename=lmdb-0.98-cp36-cp36m-linux_x86_64.whl size=218604 sha256=3cf7a872a3a4df2f1f8aeb62959b33433ab875c2b8ec83048340108164fac994\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/97/8c/7721e4b6b0ac723c6cc45ecca60599a80f75e2367330647390\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=cc90e84a16265fabfacefb2952f4bf5de99f2078afec2e6b51dd12162ab05975\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/61/df/768b03527bf006b546dce284eb4249b185669e65afc5fbb2ac\n",
            "  Building wheel for contextvars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for contextvars: filename=contextvars-2.4-cp36-none-any.whl size=7666 sha256=daebed4b60631af1ba87b5fd5d9fbde6acd55dab8da5b591b0f4cdb91f4b67f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/7d/68/1ebae2668bda2228686e3c1cf16f2c2384cea6e9334ad5f6de\n",
            "Successfully built lmdb fire contextvars\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: opencv-python, fontTools, lmdb, pillow, tenacity, fire, immutables, contextvars, aiocontextvars, loguru, sphinxcontrib-htmlhelp, sphinxcontrib-jsmath, sphinxcontrib-applehelp, sphinxcontrib-qthelp, sphinxcontrib-devhelp, sphinx, sphinx-rtd-theme\n",
            "  Found existing installation: opencv-python 4.1.2.30\n",
            "    Uninstalling opencv-python-4.1.2.30:\n",
            "      Successfully uninstalled opencv-python-4.1.2.30\n",
            "  Found existing installation: lmdb 0.99\n",
            "    Uninstalling lmdb-0.99:\n",
            "      Successfully uninstalled lmdb-0.99\n",
            "  Found existing installation: Pillow 7.0.0\n",
            "    Uninstalling Pillow-7.0.0:\n",
            "      Successfully uninstalled Pillow-7.0.0\n",
            "  Found existing installation: Sphinx 1.8.5\n",
            "    Uninstalling Sphinx-1.8.5:\n",
            "      Successfully uninstalled Sphinx-1.8.5\n",
            "Successfully installed aiocontextvars-0.2.2 contextvars-2.4 fire-0.3.1 fontTools-4.12.1 immutables-0.14 lmdb-0.98 loguru-0.5.1 opencv-python-3.4.5.20 pillow-6.2.2 sphinx-3.1.2 sphinx-rtd-theme-0.5.0 sphinxcontrib-applehelp-1.0.2 sphinxcontrib-devhelp-1.0.2 sphinxcontrib-htmlhelp-1.0.3 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.3 tenacity-6.2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "sphinxcontrib"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tqWGJwo6God",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "57a5b381-5b5d-41c6-a106-ff3d62d08422"
      },
      "source": [
        "!python main.py --help"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: main.py [-h] --config CONFIG [--dataset {lmdb,img}]\n",
            "               [--num_processes NUM_PROCESSES] [--log_period LOG_PERIOD]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --config CONFIG       python file path\n",
            "  --dataset {lmdb,img}\n",
            "  --num_processes NUM_PROCESSES\n",
            "  --log_period LOG_PERIOD\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x49C3qavAaHj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8db905de-d843-4d1f-9630-15bce9908f95"
      },
      "source": [
        "!python main.py \\\n",
        "    --config example_data/example.py \\\n",
        "    --dataset img \\\n",
        "    --num_processes 1 \\\n",
        "    --log_period 10"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m2020-08-28 16:27:11.137\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtext_renderer.corpus.char_corpus\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mload: /content/text_renderer/example_data/text/chn_text.txt\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:11.138\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtext_renderer.corpus.char_corpus\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mload: /content/text_renderer/example_data/text/eng_text.txt\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:11.138\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtext_renderer.utils.utils\u001b[0m:\u001b[36mload_chars_file\u001b[0m:\u001b[36m162\u001b[0m - \u001b[1mFind space in line 0 when load /content/text_renderer/example_data/char/chn.txt\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:11.138\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtext_renderer.utils.utils\u001b[0m:\u001b[36mload_chars_file\u001b[0m:\u001b[36m172\u001b[0m - \u001b[1mload 37 chars from: /content/text_renderer/example_data/char/chn.txt\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:11.138\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtext_renderer.corpus.corpus\u001b[0m:\u001b[36mfilter_by_chars\u001b[0m:\u001b[36m122\u001b[0m - \u001b[1mfiltering text by chars...\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:11.139\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtext_renderer.corpus.corpus\u001b[0m:\u001b[36mfilter_by_chars\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mFilter 0.12%(2) chars in input text。Unique chars(1): {'\\n'}\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:11.323\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtext_renderer.utils.utils\u001b[0m:\u001b[36mload_chars_file\u001b[0m:\u001b[36m162\u001b[0m - \u001b[1mFind space in line 0 when load /content/text_renderer/example_data/char/eng.txt\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:11.323\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtext_renderer.utils.utils\u001b[0m:\u001b[36mload_chars_file\u001b[0m:\u001b[36m172\u001b[0m - \u001b[1mload 37 chars from: /content/text_renderer/example_data/char/eng.txt\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:11.323\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtext_renderer.corpus.corpus\u001b[0m:\u001b[36mfilter_by_chars\u001b[0m:\u001b[36m122\u001b[0m - \u001b[1mfiltering text by chars...\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:11.324\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtext_renderer.corpus.corpus\u001b[0m:\u001b[36mfilter_by_chars\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mFilter 0.00%(0) chars in input text。Unique chars(0): set()\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:11.329\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtext_renderer.corpus.word_corpus\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mLoad 158 words\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:11.418\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtext_renderer.utils.utils\u001b[0m:\u001b[36mload_chars_file\u001b[0m:\u001b[36m162\u001b[0m - \u001b[1mFind space in line 0 when load /content/text_renderer/example_data/char/chn.txt\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:11.419\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtext_renderer.utils.utils\u001b[0m:\u001b[36mload_chars_file\u001b[0m:\u001b[36m172\u001b[0m - \u001b[1mload 37 chars from: /content/text_renderer/example_data/char/chn.txt\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:11.419\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtext_renderer.corpus.corpus\u001b[0m:\u001b[36mfilter_by_chars\u001b[0m:\u001b[36m122\u001b[0m - \u001b[1mfiltering text by chars...\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:11.419\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtext_renderer.corpus.corpus\u001b[0m:\u001b[36mfilter_by_chars\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mFilter 0.00%(0) chars in input text。Unique chars(0): set()\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:11.549\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtext_renderer.corpus.char_corpus\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mload: /content/text_renderer/example_data/text/chn_text.txt\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:11.549\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtext_renderer.corpus.char_corpus\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mload: /content/text_renderer/example_data/text/eng_text.txt\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:11.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtext_renderer.utils.utils\u001b[0m:\u001b[36mload_chars_file\u001b[0m:\u001b[36m162\u001b[0m - \u001b[1mFind space in line 0 when load /content/text_renderer/example_data/char/chn.txt\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:11.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtext_renderer.utils.utils\u001b[0m:\u001b[36mload_chars_file\u001b[0m:\u001b[36m172\u001b[0m - \u001b[1mload 37 chars from: /content/text_renderer/example_data/char/chn.txt\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:11.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtext_renderer.corpus.corpus\u001b[0m:\u001b[36mfilter_by_chars\u001b[0m:\u001b[36m122\u001b[0m - \u001b[1mfiltering text by chars...\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:11.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtext_renderer.corpus.corpus\u001b[0m:\u001b[36mfilter_by_chars\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mFilter 0.12%(2) chars in input text。Unique chars(1): {'\\n'}\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:11.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtext_renderer.corpus.char_corpus\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mload: /content/text_renderer/example_data/text/chn_text.txt\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:11.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtext_renderer.corpus.char_corpus\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mload: /content/text_renderer/example_data/text/eng_text.txt\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:11.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtext_renderer.utils.utils\u001b[0m:\u001b[36mload_chars_file\u001b[0m:\u001b[36m162\u001b[0m - \u001b[1mFind space in line 0 when load /content/text_renderer/example_data/char/chn.txt\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:11.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtext_renderer.utils.utils\u001b[0m:\u001b[36mload_chars_file\u001b[0m:\u001b[36m172\u001b[0m - \u001b[1mload 37 chars from: /content/text_renderer/example_data/char/chn.txt\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:11.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtext_renderer.corpus.corpus\u001b[0m:\u001b[36mfilter_by_chars\u001b[0m:\u001b[36m122\u001b[0m - \u001b[1mfiltering text by chars...\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:11.651\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtext_renderer.corpus.corpus\u001b[0m:\u001b[36mfilter_by_chars\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mFilter 0.12%(2) chars in input text。Unique chars(1): {'\\n'}\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:11.748\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtext_renderer.corpus.char_corpus\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mload: /content/text_renderer/example_data/text/chn_text.txt\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:11.749\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtext_renderer.corpus.char_corpus\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mload: /content/text_renderer/example_data/text/eng_text.txt\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:11.749\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtext_renderer.utils.utils\u001b[0m:\u001b[36mload_chars_file\u001b[0m:\u001b[36m162\u001b[0m - \u001b[1mFind space in line 0 when load /content/text_renderer/example_data/char/chn.txt\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:11.749\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtext_renderer.utils.utils\u001b[0m:\u001b[36mload_chars_file\u001b[0m:\u001b[36m172\u001b[0m - \u001b[1mload 37 chars from: /content/text_renderer/example_data/char/chn.txt\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:11.749\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtext_renderer.corpus.corpus\u001b[0m:\u001b[36mfilter_by_chars\u001b[0m:\u001b[36m122\u001b[0m - \u001b[1mfiltering text by chars...\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:11.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtext_renderer.corpus.corpus\u001b[0m:\u001b[36mfilter_by_chars\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mFilter 0.12%(2) chars in input text。Unique chars(1): {'\\n'}\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:12.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__mp_main__\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mExist image count in /content/text_renderer/example_data/output/char_corpus: 0\u001b[0m\n",
            "\u001b[32m2020-08-28 16:27:12.462\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__mp_main__\u001b[0m:\u001b[36mprocess_setup\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mFinish setup image generate process: 226\u001b[0m\n",
            "\u001b[32m2020-08-28 16:28:48.173\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__mp_main__\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m52\u001b[0m - \u001b[1mDBWriterProcess receive stop token\u001b[0m\n",
            "\u001b[32m2020-08-28 16:28:48.173\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__mp_main__\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m62\u001b[0m - \u001b[1m100.00%(10000/10000)\u001b[0m\n",
            "\u001b[32m2020-08-28 16:28:48.174\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__mp_main__\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mFinish generate: 10000. Total: 10000\u001b[0m\n",
            "\u001b[32m2020-08-28 16:28:48.615\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__mp_main__\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mExist image count in /content/text_renderer/example_data/output/rand_corpus: 0\u001b[0m\n",
            "\u001b[32m2020-08-28 16:28:48.981\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__mp_main__\u001b[0m:\u001b[36mprocess_setup\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mFinish setup image generate process: 30264\u001b[0m\n",
            "\u001b[32m2020-08-28 16:29:50.432\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__mp_main__\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m52\u001b[0m - \u001b[1mDBWriterProcess receive stop token\u001b[0m\n",
            "\u001b[32m2020-08-28 16:29:50.433\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__mp_main__\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m62\u001b[0m - \u001b[1m200.00%(20000/10000)\u001b[0m\n",
            "\u001b[32m2020-08-28 16:29:50.433\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__mp_main__\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mFinish generate: 20000. Total: 20000\u001b[0m\n",
            "\u001b[32m2020-08-28 16:29:50.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__mp_main__\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mExist image count in /content/text_renderer/example_data/output/word_corpus: 0\u001b[0m\n",
            "\u001b[32m2020-08-28 16:29:51.240\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__mp_main__\u001b[0m:\u001b[36mprocess_setup\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mFinish setup image generate process: 60292\u001b[0m\n",
            "\u001b[32m2020-08-28 16:31:19.368\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__mp_main__\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m52\u001b[0m - \u001b[1mDBWriterProcess receive stop token\u001b[0m\n",
            "\u001b[32m2020-08-28 16:31:19.368\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__mp_main__\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m62\u001b[0m - \u001b[1m300.00%(30000/10000)\u001b[0m\n",
            "\u001b[32m2020-08-28 16:31:19.368\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__mp_main__\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mFinish generate: 30000. Total: 30000\u001b[0m\n",
            "\u001b[32m2020-08-28 16:31:19.855\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__mp_main__\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mExist image count in /content/text_renderer/example_data/output/same_line_data: 0\u001b[0m\n",
            "\u001b[32m2020-08-28 16:31:20.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__mp_main__\u001b[0m:\u001b[36mprocess_setup\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mFinish setup image generate process: 90326\u001b[0m\n",
            "\u001b[32m2020-08-28 16:34:56.006\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__mp_main__\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m52\u001b[0m - \u001b[1mDBWriterProcess receive stop token\u001b[0m\n",
            "\u001b[32m2020-08-28 16:34:56.007\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__mp_main__\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m62\u001b[0m - \u001b[1m400.00%(40000/10000)\u001b[0m\n",
            "\u001b[32m2020-08-28 16:34:56.007\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__mp_main__\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mFinish generate: 40000. Total: 40000\u001b[0m\n",
            "\u001b[32m2020-08-28 16:34:56.499\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__mp_main__\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mExist image count in /content/text_renderer/example_data/output/extra_text_line_data: 0\u001b[0m\n",
            "\u001b[32m2020-08-28 16:34:56.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__mp_main__\u001b[0m:\u001b[36mprocess_setup\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mFinish setup image generate process: 120399\u001b[0m\n",
            "\u001b[32m2020-08-28 16:36:37.536\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__mp_main__\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m52\u001b[0m - \u001b[1mDBWriterProcess receive stop token\u001b[0m\n",
            "\u001b[32m2020-08-28 16:36:37.536\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__mp_main__\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m62\u001b[0m - \u001b[1m500.00%(50000/10000)\u001b[0m\n",
            "\u001b[32m2020-08-28 16:36:37.537\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__mp_main__\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mFinish generate: 50000. Total: 50000\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izVrU4o8bOy0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json \n",
        "import csv \n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_json ('/content/text_renderer/example_data/output/word_corpus/labels.json')  \n",
        "df.to_csv ('/content/text_renderer/data_sample.csv', index = None, header=True)  \n",
        "df_1 = pd.read_json('/content/text_renderer/example_data/output/char_corpus/labels.json')\n",
        "df_1.to_csv ('/content/text_renderer/data_sample_1.csv', index = None, header=True)  \n",
        "df_2 = pd.read_json ('/content/text_renderer/example_data/output/extra_text_line_data/labels.json')  \n",
        "df_2.to_csv ('/content/text_renderer/data_sample_2.csv', index = None, header=True)  \n",
        "df_3 = pd.read_json('/content/text_renderer/example_data/output/rand_corpus/labels.json')\n",
        "df_3.to_csv ('/content/text_renderer/data_sample_3.csv', index = None, header=True)  \n",
        "df_4 = pd.read_json('/content/text_renderer/example_data/output/same_line_data/labels.json')\n",
        "df_4.to_csv ('/content/text_renderer/data_sample_4.csv', index = None, header=True)  "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmZy0GDzTFvh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "439d19a9-bc04-4546-a9a5-66445d1fece7"
      },
      "source": [
        "df_3"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num-samples</th>\n",
              "      <th>labels</th>\n",
              "      <th>sizes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10000</th>\n",
              "      <td>20000</td>\n",
              "      <td>7205924</td>\n",
              "      <td>[136, 32]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10001</th>\n",
              "      <td>20000</td>\n",
              "      <td>15078</td>\n",
              "      <td>[97, 32]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10002</th>\n",
              "      <td>20000</td>\n",
              "      <td>407538</td>\n",
              "      <td>[152, 32]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10003</th>\n",
              "      <td>20000</td>\n",
              "      <td>36393880</td>\n",
              "      <td>[175, 32]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10004</th>\n",
              "      <td>20000</td>\n",
              "      <td>40091758</td>\n",
              "      <td>[160, 32]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19995</th>\n",
              "      <td>20000</td>\n",
              "      <td>99029</td>\n",
              "      <td>[112, 32]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19996</th>\n",
              "      <td>20000</td>\n",
              "      <td>2582214</td>\n",
              "      <td>[164, 32]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19997</th>\n",
              "      <td>20000</td>\n",
              "      <td>418690720</td>\n",
              "      <td>[181, 32]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19998</th>\n",
              "      <td>20000</td>\n",
              "      <td>6277900</td>\n",
              "      <td>[134, 32]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19999</th>\n",
              "      <td>20000</td>\n",
              "      <td>77100</td>\n",
              "      <td>[112, 32]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       num-samples     labels      sizes\n",
              "10000        20000    7205924  [136, 32]\n",
              "10001        20000      15078   [97, 32]\n",
              "10002        20000     407538  [152, 32]\n",
              "10003        20000   36393880  [175, 32]\n",
              "10004        20000   40091758  [160, 32]\n",
              "...            ...        ...        ...\n",
              "19995        20000      99029  [112, 32]\n",
              "19996        20000    2582214  [164, 32]\n",
              "19997        20000  418690720  [181, 32]\n",
              "19998        20000    6277900  [134, 32]\n",
              "19999        20000      77100  [112, 32]\n",
              "\n",
              "[10000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udje1O1XObch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.concat([df_1, df_3, df, df_4, df_2], axis = 0)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqPSTpVLTuuC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.drop(['num-samples'], axis = 1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyUGf8xIT7ka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['image_name'] = df.index"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScYRd94bPkIt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "69950161-7546-4cd4-bf98-66b93a839434"
      },
      "source": [
        "df"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>labels</th>\n",
              "      <th>sizes</th>\n",
              "      <th>image_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>O HEADED</td>\n",
              "      <td>[246, 32]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ICH IT HA</td>\n",
              "      <td>[273, 32]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>E OF AL</td>\n",
              "      <td>[200, 32]</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>EDITIO</td>\n",
              "      <td>[241, 32]</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>HAS DO</td>\n",
              "      <td>[182, 32]</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>ACANT GUL</td>\n",
              "      <td>[144, 32]</td>\n",
              "      <td>49995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>THER THIS</td>\n",
              "      <td>[156, 32]</td>\n",
              "      <td>49996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>OPS THE B</td>\n",
              "      <td>[162, 32]</td>\n",
              "      <td>49997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>SING AS A</td>\n",
              "      <td>[151, 32]</td>\n",
              "      <td>49998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>BSESSED W</td>\n",
              "      <td>[178, 32]</td>\n",
              "      <td>49999</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          labels      sizes  image_name\n",
              "0      O HEADED   [246, 32]           0\n",
              "1      ICH IT HA  [273, 32]           1\n",
              "2        E OF AL  [200, 32]           2\n",
              "3         EDITIO  [241, 32]           3\n",
              "4         HAS DO  [182, 32]           4\n",
              "...          ...        ...         ...\n",
              "49995  ACANT GUL  [144, 32]       49995\n",
              "49996  THER THIS  [156, 32]       49996\n",
              "49997  OPS THE B  [162, 32]       49997\n",
              "49998  SING AS A  [151, 32]       49998\n",
              "49999  BSESSED W  [178, 32]       49999\n",
              "\n",
              "[50000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lPIz3bf67vb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "ab6fe539-7aff-4f68-83ae-9f49912533a8"
      },
      "source": [
        "for i in range(len(df.labels)):\n",
        "  if type(df.labels[i]) == int:\n",
        "   df.labels[i] = str(df.labels[i])\n",
        "   continue\n",
        "  df.labels[i] = df.labels[i].replace(' ', '')\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j90_DdTWkSfq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "32ce3edf-8952-4343-c8a9-dc865d1af294"
      },
      "source": [
        "type(df.labels[10001])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3sakxlweL4g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _bytes_feature(value):\n",
        "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
        "  if isinstance(value, type(tf.constant(0))):\n",
        "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
        "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "def _float_feature(value):\n",
        "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
        "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
        "def _int64_feature(value):\n",
        "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
        "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
        "def _int64_list_feature(value):\n",
        "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
        "  return tf.train.Feature(int64_list=tf.train.Int64List(value= value))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eu1vM6Lurp8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np, pandas as pd, os\n",
        "import matplotlib.pyplot as plt, cv2\n",
        "import tensorflow as tf, re, math\n",
        "from skimage import io\n",
        "import imghdr"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RbP1sCY3zmz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getListOfFiles(dirName):\n",
        "    # create a list of file and sub directories \n",
        "    # names in the given directory \n",
        "    listOfFile = os.listdir(dirName)\n",
        "    allFiles = list()\n",
        "    # Iterate over all the entries\n",
        "    for entry in listOfFile:\n",
        "        # Create full path\n",
        "        fullPath = os.path.join(dirName, entry)\n",
        "        # If entry is a directory then get the list of files in this directory \n",
        "        if os.path.isdir(fullPath):\n",
        "            allFiles = allFiles + getListOfFiles(fullPath)\n",
        "        else:\n",
        "            allFiles.append(fullPath)\n",
        "                \n",
        "    return allFiles\n",
        "IMGS = getListOfFiles('/content/text_renderer/example_data/output')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAQ7DYL5q_EQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "b1f4d617-b017-4fa5-fdfc-c4750bb27504"
      },
      "source": [
        "def serialize_example(feature0, feature1, feature2, feature3):\n",
        "  feature = {\n",
        "      'image': _bytes_feature(feature0),\n",
        "      'image_name': _int64_feature(feature1),\n",
        "      'sizes': _int64_list_feature(feature2),\n",
        "      'labels': _bytes_feature(feature3)\n",
        "  }\n",
        "  example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "  return example_proto.SerializeToString()\n",
        "SIZE = 10000\n",
        "CT = len(IMGS)//SIZE + int(len(IMGS)%SIZE!=0)\n",
        "for j in range(CT):\n",
        "    print(); print('Writing TFRecord %i of %i...'%(j,CT))\n",
        "    CT2 = min(SIZE,len(IMGS)-j*SIZE)\n",
        "    with tf.io.TFRecordWriter('/content/gdrive/My Drive/train%.2i-%i.tfrec'%(j,CT2)) as writer:\n",
        "        for k in range(CT2):\n",
        "            if  imghdr.what(IMGS[k]) == None:\n",
        "              continue\n",
        "            img = cv2.imread(IMGS[k])\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) # Fix incorrect colors\n",
        "            img = cv2.imencode('.jpg', img, (cv2.IMWRITE_JPEG_QUALITY, 94))[1].tostring()\n",
        "            name = IMGS[SIZE*j+k].split('.')[0]\n",
        "            row = df.iloc[k]\n",
        "            example = serialize_example(\n",
        "                img, row.image_name,\n",
        "                row.sizes,\n",
        "                str.encode(row.labels))\n",
        "            writer.write(example)\n",
        "            if k%100==0: print(k,', ',end='')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Writing TFRecord 0 of 6...\n",
            "100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , 1600 , 1700 , 1800 , 1900 , 2000 , 2100 , 2200 , 2300 , 2400 , 2500 , 2600 , 2700 , 2800 , 2900 , 3000 , 3100 , 3200 , 3300 , 3400 , 3500 , 3600 , 3700 , 3800 , 3900 , 4000 , 4100 , 4200 , 4300 , 4400 , 4500 , 4600 , 4700 , 4800 , 4900 , 5000 , 5100 , 5200 , 5300 , 5400 , 5500 , 5600 , 5700 , 5800 , 5900 , 6000 , 6100 , 6200 , 6300 , 6400 , 6500 , 6600 , 6700 , 6800 , 6900 , 7000 , 7100 , 7200 , 7300 , 7400 , 7500 , 7600 , 7700 , 7800 , 7900 , 8000 , 8100 , 8200 , 8300 , 8400 , 8500 , 8600 , 8700 , 8800 , 8900 , 9000 , 9100 , 9200 , 9300 , 9400 , 9500 , 9600 , 9700 , 9800 , 9900 , \n",
            "Writing TFRecord 1 of 6...\n",
            "100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , 1600 , 1700 , 1800 , 1900 , 2000 , 2100 , 2200 , 2300 , 2400 , 2500 , 2600 , 2700 , 2800 , 2900 , 3000 , 3100 , 3200 , 3300 , 3400 , 3500 , 3600 , 3700 , 3800 , 3900 , 4000 , 4100 , 4200 , 4300 , 4400 , 4500 , 4600 , 4700 , 4800 , 4900 , 5000 , 5100 , 5200 , 5300 , 5400 , 5500 , 5600 , 5700 , 5800 , 5900 , 6000 , 6100 , 6200 , 6300 , 6400 , 6500 , 6600 , 6700 , 6800 , 6900 , 7000 , 7100 , 7200 , 7300 , 7400 , 7500 , 7600 , 7700 , 7800 , 7900 , 8000 , 8100 , 8200 , 8300 , 8400 , 8500 , 8600 , 8700 , 8800 , 8900 , 9000 , 9100 , 9200 , 9300 , 9400 , 9500 , 9600 , 9700 , 9800 , 9900 , \n",
            "Writing TFRecord 2 of 6...\n",
            "100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , 1600 , 1700 , 1800 , 1900 , 2000 , 2100 , 2200 , 2300 , 2400 , 2500 , 2600 , 2700 , 2800 , 2900 , 3000 , 3100 , 3200 , 3300 , 3400 , 3500 , 3600 , 3700 , 3800 , 3900 , 4000 , 4100 , 4200 , 4300 , 4400 , 4500 , 4600 , 4700 , 4800 , 4900 , 5000 , 5100 , 5200 , 5300 , 5400 , 5500 , 5600 , 5700 , 5800 , 5900 , 6000 , 6100 , 6200 , 6300 , 6400 , 6500 , 6600 , 6700 , 6800 , 6900 , 7000 , 7100 , 7200 , 7300 , 7400 , 7500 , 7600 , 7700 , 7800 , 7900 , 8000 , 8100 , 8200 , 8300 , 8400 , 8500 , 8600 , 8700 , 8800 , 8900 , 9000 , 9100 , 9200 , 9300 , 9400 , 9500 , 9600 , 9700 , 9800 , 9900 , \n",
            "Writing TFRecord 3 of 6...\n",
            "100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , 1600 , 1700 , 1800 , 1900 , 2000 , 2100 , 2200 , 2300 , 2400 , 2500 , 2600 , 2700 , 2800 , 2900 , 3000 , 3100 , 3200 , 3300 , 3400 , 3500 , 3600 , 3700 , 3800 , 3900 , 4000 , 4100 , 4200 , 4300 , 4400 , 4500 , 4600 , 4700 , 4800 , 4900 , 5000 , 5100 , 5200 , 5300 , 5400 , 5500 , 5600 , 5700 , 5800 , 5900 , 6000 , 6100 , 6200 , 6300 , 6400 , 6500 , 6600 , 6700 , 6800 , 6900 , 7000 , 7100 , 7200 , 7300 , 7400 , 7500 , 7600 , 7700 , 7800 , 7900 , 8000 , 8100 , 8200 , 8300 , 8400 , 8500 , 8600 , 8700 , 8800 , 8900 , 9000 , 9100 , 9200 , 9300 , 9400 , 9500 , 9600 , 9700 , 9800 , 9900 , \n",
            "Writing TFRecord 4 of 6...\n",
            "100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , 1600 , 1700 , 1800 , 1900 , 2000 , 2100 , 2200 , 2300 , 2400 , 2500 , 2600 , 2700 , 2800 , 2900 , 3000 , 3100 , 3200 , 3300 , 3400 , 3500 , 3600 , 3700 , 3800 , 3900 , 4000 , 4100 , 4200 , 4300 , 4400 , 4500 , 4600 , 4700 , 4800 , 4900 , 5000 , 5100 , 5200 , 5300 , 5400 , 5500 , 5600 , 5700 , 5800 , 5900 , 6000 , 6100 , 6200 , 6300 , 6400 , 6500 , 6600 , 6700 , 6800 , 6900 , 7000 , 7100 , 7200 , 7300 , 7400 , 7500 , 7600 , 7700 , 7800 , 7900 , 8000 , 8100 , 8200 , 8300 , 8400 , 8500 , 8600 , 8700 , 8800 , 8900 , 9000 , 9100 , 9200 , 9300 , 9400 , 9500 , 9600 , 9700 , 9800 , 9900 , \n",
            "Writing TFRecord 5 of 6...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrbBuNWXnd90",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_image(image_data):\n",
        "    image = tf.image.decode_jpeg(image_data, channels=3)\n",
        "    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n",
        "    image = tf.reshape(image, [*IMAGE_SIZE]) # explicit size needed for TPU\n",
        "    print(image)\n",
        "    return image\n",
        "\n",
        "def read_labeled_tfrecord(example):\n",
        "    LABELED_TFREC_FORMAT = {\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
        "        \"image_name\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n",
        "        \"sizes\": tf.io.FixedLenSequenceFeature([], tf.int64, allow_missing=True),\n",
        "        \"labels\": tf.io.FixedLenFeature([], tf.string)\n",
        "    }\n",
        "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
        "    sizes = example['sizes']\n",
        "    image = decode_image(example['image'])\n",
        "    image_name = example['image_name']\n",
        "    labels = example['labels']\n",
        "    \n",
        "    return image, labels, image_name, sizes # returns a dataset of (image, label) pairs\n",
        "\n",
        "def load_dataset(filenames, labeled=True, ordered=False):\n",
        "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
        "    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n",
        "\n",
        "    ignore_order = tf.data.Options()\n",
        "    if not ordered:\n",
        "        ignore_order.experimental_deterministic = False # disable order, increase speed\n",
        "\n",
        "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n",
        "    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n",
        "    dataset = dataset.map(read_labeled_tfrecord)\n",
        "    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n",
        "    return dataset\n",
        "\n",
        "def get_training_dataset():\n",
        "    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)\n",
        "    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n",
        "    dataset = dataset.shuffle(2048)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "    return dataset\n",
        "\n",
        "def count_data_items(filenames):\n",
        "    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
        "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
        "    return np.sum(n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRAn1xTiobLp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "71ae5ebb-e16d-4e46-acc8-f63c71e085c6"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "IMAGE_SIZE= [220,32]\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "TRAINING_FILENAMES = tf.io.gfile.glob('/content/gdrive/My Drive/tfrecord/*.tfrec')\n",
        "print('There are %i train images'%count_data_items(TRAINING_FILENAMES))\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-06e38aeb1731>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mIMAGE_SIZE\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m220\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mAUTO\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mTRAINING_FILENAMES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/tfrecord/*.tfrec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'There are %i train images'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mcount_data_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAINING_FILENAMES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mget_matching_files_v2\u001b[0;34m(pattern)\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatching_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         for matching_filename in _pywrap_file_io.GetMatchingFiles(\n\u001b[0;32m--> 409\u001b[0;31m             compat.as_bytes(pattern))\n\u001b[0m\u001b[1;32m    410\u001b[0m     ]\n\u001b[1;32m    411\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: /content/gdrive/My Drive/tfrecord; No such file or directory"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q25b94SPooiB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0283ce0a-7f15-4b1f-a60e-8e404967858d"
      },
      "source": [
        "training_dataset = get_training_dataset()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"Reshape:0\", shape=(220, 32), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2ygtj2lorcU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "386c8a35-4b68-4e2c-fe54-1e7eadfd9203"
      },
      "source": [
        "training_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((None, 220, 32), (None,), (None,), (None, None)), types: (tf.float32, tf.string, tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NTT2_tPFIuW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "90f5a14a-94f4-4b0e-c0b2-26d2cf92bc83"
      },
      "source": [
        "%cd '/content'"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6gM8C3HxoAM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ecc108c0-bf59-47f8-c8bd-c402d9b658c7"
      },
      "source": [
        "!git clone https://github.com/sanyam83/License-plate-recognition-using-YOLOv3.git\n",
        "from keras import backend as K\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "%cd /content/License-plate-recognition-using-YOLOv3\n",
        "from crnn.Image_Generator import TextImageGenerator\n",
        "from crnn.Model import get_Model\n",
        "from crnn.parameter import *\n",
        "K.set_learning_phase(0)\n",
        "\n",
        "# # Model description and training\n",
        "\n",
        "model = get_Model(training=True)\n",
        "\n",
        "train_file_path = '/content/text_renderer/example_data/output'\n",
        "tiger_train = TextImageGenerator(train_file_path, img_w, img_h, batch_size, downsample_factor, df)\n",
        "tiger_train.build_data()\n",
        "\n",
        "\n",
        "ada = Adam(lr=1e-4)\n",
        "\n",
        "early_stop = EarlyStopping(monitor='loss', min_delta=0.001, patience=4, mode='min', verbose=1)\n",
        "checkpoint = ModelCheckpoint(filepath='LSTM+BN5--{epoch:02d}--{loss:.3f}.hdf5', monitor='loss', verbose=1, mode='min', period=1)\n",
        "# the loss calc occurs elsewhere, so use a dummy lambda func for the loss\n",
        "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=ada)\n",
        "\n",
        "# captures output of softmax so we can decode the output during visualization\n",
        "model.fit_generator(generator=tiger_train.next_batch(),\n",
        "                    steps_per_epoch=int(tiger_train.n / batch_size),\n",
        "                    epochs=30,\n",
        "                    callbacks=[checkpoint])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/License-plate-recognition-using-YOLOv3\n",
            "WARNING:tensorflow:From /content/License-plate-recognition-using-YOLOv3/crnn/Model.py:9: set_learning_phase (from tensorflow.python.keras.backend) is deprecated and will be removed after 2020-10-11.\n",
            "Instructions for updating:\n",
            "Simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "50000  Image Loading start...\n",
            "True\n",
            "50000  Image Loading finish...\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "WARNING:tensorflow:From <ipython-input-11-81c5cc704545>:30: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/30\n",
            "390/390 [==============================] - ETA: 0s - loss: 39.2715\n",
            "Epoch 00001: saving model to LSTM+BN5--01--39.272.hdf5\n",
            "390/390 [==============================] - 88s 226ms/step - loss: 39.2715\n",
            "Epoch 2/30\n",
            "390/390 [==============================] - ETA: 0s - loss: 26.9811\n",
            "Epoch 00002: saving model to LSTM+BN5--02--26.981.hdf5\n",
            "390/390 [==============================] - 89s 229ms/step - loss: 26.9811\n",
            "Epoch 3/30\n",
            "390/390 [==============================] - ETA: 0s - loss: 24.4892\n",
            "Epoch 00003: saving model to LSTM+BN5--03--24.489.hdf5\n",
            "390/390 [==============================] - 90s 230ms/step - loss: 24.4892\n",
            "Epoch 4/30\n",
            "390/390 [==============================] - ETA: 0s - loss: 23.8760\n",
            "Epoch 00004: saving model to LSTM+BN5--04--23.876.hdf5\n",
            "390/390 [==============================] - 89s 229ms/step - loss: 23.8760\n",
            "Epoch 5/30\n",
            "390/390 [==============================] - ETA: 0s - loss: 23.5619\n",
            "Epoch 00005: saving model to LSTM+BN5--05--23.562.hdf5\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 23.5619\n",
            "Epoch 6/30\n",
            "390/390 [==============================] - ETA: 0s - loss: 23.2521\n",
            "Epoch 00006: saving model to LSTM+BN5--06--23.252.hdf5\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 23.2521\n",
            "Epoch 7/30\n",
            "390/390 [==============================] - ETA: 0s - loss: 23.0756\n",
            "Epoch 00007: saving model to LSTM+BN5--07--23.076.hdf5\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 23.0756\n",
            "Epoch 8/30\n",
            "390/390 [==============================] - ETA: 0s - loss: 22.9394\n",
            "Epoch 00008: saving model to LSTM+BN5--08--22.939.hdf5\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 22.9394\n",
            "Epoch 9/30\n",
            "390/390 [==============================] - ETA: 0s - loss: 22.7703\n",
            "Epoch 00009: saving model to LSTM+BN5--09--22.770.hdf5\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 22.7703\n",
            "Epoch 10/30\n",
            "390/390 [==============================] - ETA: 0s - loss: 22.6411\n",
            "Epoch 00010: saving model to LSTM+BN5--10--22.641.hdf5\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 22.6411\n",
            "Epoch 11/30\n",
            "390/390 [==============================] - ETA: 0s - loss: 22.4604\n",
            "Epoch 00011: saving model to LSTM+BN5--11--22.460.hdf5\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 22.4604\n",
            "Epoch 12/30\n",
            "390/390 [==============================] - ETA: 0s - loss: 22.2336\n",
            "Epoch 00012: saving model to LSTM+BN5--12--22.234.hdf5\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 22.2336\n",
            "Epoch 13/30\n",
            "390/390 [==============================] - ETA: 0s - loss: 22.1094\n",
            "Epoch 00013: saving model to LSTM+BN5--13--22.109.hdf5\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 22.1094\n",
            "Epoch 14/30\n",
            "390/390 [==============================] - ETA: 0s - loss: 21.9520\n",
            "Epoch 00014: saving model to LSTM+BN5--14--21.952.hdf5\n",
            "390/390 [==============================] - 89s 229ms/step - loss: 21.9520\n",
            "Epoch 15/30\n",
            "390/390 [==============================] - ETA: 0s - loss: 21.6610\n",
            "Epoch 00015: saving model to LSTM+BN5--15--21.661.hdf5\n",
            "390/390 [==============================] - 89s 229ms/step - loss: 21.6610\n",
            "Epoch 16/30\n",
            "390/390 [==============================] - ETA: 0s - loss: 21.2504\n",
            "Epoch 00016: saving model to LSTM+BN5--16--21.250.hdf5\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 21.2504\n",
            "Epoch 17/30\n",
            "390/390 [==============================] - ETA: 0s - loss: 21.0310\n",
            "Epoch 00017: saving model to LSTM+BN5--17--21.031.hdf5\n",
            "390/390 [==============================] - 90s 230ms/step - loss: 21.0310\n",
            "Epoch 18/30\n",
            "390/390 [==============================] - ETA: 0s - loss: 20.5415\n",
            "Epoch 00018: saving model to LSTM+BN5--18--20.542.hdf5\n",
            "390/390 [==============================] - 89s 229ms/step - loss: 20.5415\n",
            "Epoch 19/30\n",
            "390/390 [==============================] - ETA: 0s - loss: 19.9775\n",
            "Epoch 00019: saving model to LSTM+BN5--19--19.978.hdf5\n",
            "390/390 [==============================] - 89s 228ms/step - loss: 19.9775\n",
            "Epoch 20/30\n",
            "390/390 [==============================] - ETA: 0s - loss: 19.1960\n",
            "Epoch 00020: saving model to LSTM+BN5--20--19.196.hdf5\n",
            "390/390 [==============================] - 89s 227ms/step - loss: 19.1960\n",
            "Epoch 21/30\n",
            "390/390 [==============================] - ETA: 0s - loss: 18.1491\n",
            "Epoch 00021: saving model to LSTM+BN5--21--18.149.hdf5\n",
            "390/390 [==============================] - 89s 228ms/step - loss: 18.1491\n",
            "Epoch 22/30\n",
            "390/390 [==============================] - ETA: 0s - loss: 17.3604\n",
            "Epoch 00022: saving model to LSTM+BN5--22--17.360.hdf5\n",
            "390/390 [==============================] - 88s 226ms/step - loss: 17.3604\n",
            "Epoch 23/30\n",
            "390/390 [==============================] - ETA: 0s - loss: 16.3170\n",
            "Epoch 00023: saving model to LSTM+BN5--23--16.317.hdf5\n",
            "390/390 [==============================] - 89s 228ms/step - loss: 16.3170\n",
            "Epoch 24/30\n",
            "390/390 [==============================] - ETA: 0s - loss: 15.1284\n",
            "Epoch 00024: saving model to LSTM+BN5--24--15.128.hdf5\n",
            "390/390 [==============================] - 88s 226ms/step - loss: 15.1284\n",
            "Epoch 25/30\n",
            "390/390 [==============================] - ETA: 0s - loss: 14.1440\n",
            "Epoch 00025: saving model to LSTM+BN5--25--14.144.hdf5\n",
            "390/390 [==============================] - 88s 226ms/step - loss: 14.1440\n",
            "Epoch 26/30\n",
            "390/390 [==============================] - ETA: 0s - loss: 13.1501\n",
            "Epoch 00026: saving model to LSTM+BN5--26--13.150.hdf5\n",
            "390/390 [==============================] - 88s 226ms/step - loss: 13.1501\n",
            "Epoch 27/30\n",
            "390/390 [==============================] - ETA: 0s - loss: 11.9626\n",
            "Epoch 00027: saving model to LSTM+BN5--27--11.963.hdf5\n",
            "390/390 [==============================] - 88s 226ms/step - loss: 11.9626\n",
            "Epoch 28/30\n",
            "390/390 [==============================] - ETA: 0s - loss: 10.9565\n",
            "Epoch 00028: saving model to LSTM+BN5--28--10.957.hdf5\n",
            "390/390 [==============================] - 88s 225ms/step - loss: 10.9565\n",
            "Epoch 29/30\n",
            "390/390 [==============================] - ETA: 0s - loss: 10.0692\n",
            "Epoch 00029: saving model to LSTM+BN5--29--10.069.hdf5\n",
            "390/390 [==============================] - 88s 226ms/step - loss: 10.0692\n",
            "Epoch 30/30\n",
            "390/390 [==============================] - ETA: 0s - loss: 9.0259\n",
            "Epoch 00030: saving model to LSTM+BN5--30--9.026.hdf5\n",
            "390/390 [==============================] - 88s 226ms/step - loss: 9.0259\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f564078c3c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nb3XDl79nEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import itertools, os, time\n",
        "import numpy as np\n",
        "from crnn.Model import get_Model\n",
        "from crnn.parameter import letters\n",
        "import argparse\n",
        "from keras import backend as K\n",
        "K.set_learning_phase(0)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ot2-osHEN-SC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = get_Model(training=False)\n",
        "model.load_weights('/content/License-plate-recognition-using-YOLOv3/LSTM+BN5--30--9.026.hdf5')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHnv91QmOdj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_label(out):\n",
        "    # out : (1, 32, 42)\n",
        "    out_best = list(np.argmax(out[0, 2:], axis=1))  # get max index -> len = 32\n",
        "    out_best = [k for k, g in itertools.groupby(out_best)]  # remove overlap value\n",
        "    outstr = ''\n",
        "    for i in out_best:\n",
        "        if i < len(letters):\n",
        "            outstr += letters[i]\n",
        "    return outstr"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPlTwpxCzb88",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7e977a38-dfb9-48e8-9708-c545d225bb32"
      },
      "source": [
        "img = cv2.imread('/content/84.png', cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "img_pred = img.astype(np.float32)\n",
        "img_pred = cv2.resize(img_pred, (32, 128))\n",
        "img_pred = (img_pred / 255.0) * 2.0 - 1.0\n",
        "img_pred = np.expand_dims(img_pred, axis=-1)\n",
        "img_pred = np.expand_dims(img_pred, axis=0)\n",
        "print(img_pred.shape)\n",
        "net_out_value = model.predict(img_pred)\n",
        "pred_texts = decode_label(net_out_value)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 128, 32, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcUZRDGiOtcY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}